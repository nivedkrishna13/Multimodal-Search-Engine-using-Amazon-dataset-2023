{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":14911925,"datasetId":9541633,"databundleVersionId":15777692}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import CLIPProcessor, CLIPModel\nfrom io import BytesIO\nimport pyarrow.parquet as pq\n# Paths\nDATA_PATH = \"/kaggle/input/datasets/nivedkrishna13/amazon-2023-reduced-parquet/amazon_clean.parquet\"\nOUTPUT_DIR = \"embeddings\"\nBATCH_SIZE = 64\nCHUNK_SIZE = 500  # rows read at once (memory safe)\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\n# Load CLIP\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel.eval()\n\n\ndef download_image(url):\n    try:\n        response = requests.get(url, timeout=5)\n        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        return img\n    except:\n        return None\n\n\ndef process_chunk(df_chunk, chunk_id):\n    image_embeddings = []\n    text_embeddings = []\n    valid_ids = []\n\n    for start in tqdm(range(0, len(df_chunk), BATCH_SIZE)):\n        batch = df_chunk.iloc[start:start + BATCH_SIZE]\n\n        images = []\n        texts = []\n        ids = []\n\n        for _, row in batch.iterrows():\n            img = download_image(row[\"imgUrl\"])\n            if img is None:\n                continue\n            images.append(img)\n            texts.append(row[\"title\"])\n            ids.append(row[\"asin\"])\n\n            if len(images) == 0:\n                continue\n\n            try:\n                inputs = processor(\n                    text=texts,\n                    images=images,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    truncation=True,    # Crucial fix\n                    max_length=77       # CLIP limit\n                ).to(device)\n\n                with torch.no_grad():\n                    outputs = model(**inputs)\n                    # These are the projected latent vectors\n                    img_emb = outputs.image_embeds\n                    txt_emb = outputs.text_embeds\n\n                    # L2 Normalization for Cosine Similarity\n                    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n                    txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n            except Exception as e:\n                print(f\"Error processing batch in chunk {chunk_id}: {e}\")\n                continue\n        image_embeddings.append(img_emb.cpu().numpy())\n        text_embeddings.append(txt_emb.cpu().numpy())\n        valid_ids.extend(ids)\n\n        torch.cuda.empty_cache()\n\n    if len(image_embeddings) == 0:\n        return\n\n    image_embeddings = np.vstack(image_embeddings)\n    text_embeddings = np.vstack(text_embeddings)\n\n    np.save(f\"{OUTPUT_DIR}/image_emb_chunk_{chunk_id}.npy\", image_embeddings)\n    np.save(f\"{OUTPUT_DIR}/text_emb_chunk_{chunk_id}.npy\", text_embeddings)\n\n    pd.Series(valid_ids).to_csv(\n        f\"{OUTPUT_DIR}/ids_chunk_{chunk_id}.csv\",\n        index=False\n    )\n\n    print(f\"Saved chunk {chunk_id}\")\n\n\n\ndef main():\n    print(\"Reading dataset in chunks...\")\n    \n    # Create a ParquetFile object\n    parquet_file = pq.ParquetFile(DATA_PATH)\n    \n    # Iterate through row groups (Parquet's internal \"chunks\")\n    for i in range(parquet_file.num_row_groups):\n        print(f\"\\nProcessing chunk (row group) {i}\")\n        \n        # Read one row group into a pandas DataFrame\n        df_chunk = parquet_file.read_row_group(i).to_pandas()\n        \n        process_chunk(df_chunk, i)\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-22T07:19:30.794110Z","iopub.execute_input":"2026-02-22T07:19:30.794302Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c98119f3991454baede1167dac92949"}},"metadata":{}},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a310a0cba4fa48f1b98c63fb3a900790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5deac23110794ecc82a4333104333d47"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\nKey                                  | Status     |  | \n-------------------------------------+------------+--+-\ntext_model.embeddings.position_ids   | UNEXPECTED |  | \nvision_model.embeddings.position_ids | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b97e501968c4891aba7d16a217252f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868a0025b27c4fa9830b95a43c1a6c69"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33def1f6e7ed4692b3e63a6953fbce7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9498b3215f14a0b87ea8008af18b1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ccd189fdab0479285fa2b3bf6a540f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc21db65a8d94b92a1e408d2a0495f68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2125215d1e4d7da27cd6ed553e188b"}},"metadata":{}},{"name":"stdout","text":"Reading dataset in chunks...\n\nProcessing chunk (row group) 0\n","output_type":"stream"},{"name":"stderr","text":"  2%|â–         | 59/3104 [16:34<15:07:57, 17.89s/it]","output_type":"stream"}],"execution_count":null}]}